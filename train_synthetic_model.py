#!/usr/bin/env python3
"""
Sentetik Dataset ile Model EÄŸitimi
Ses Ã¶zelliklerini Ã§Ä±karÄ±r ve Ã§oklu ML modelleri eÄŸitir
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from datetime import datetime

# ML kÃ¼tÃ¼phaneleri
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline

# GÃ¶rselleÅŸtirme
import matplotlib.pyplot as plt
import seaborn as sns

# Ses iÅŸleme
from feature_extraction import AudioFeatureExtractor

class SyntheticModelTrainer:
    """Sentetik dataset ile model eÄŸitici"""
    
    def __init__(self, dataset_dir):
        """
        Args:
            dataset_dir (str): Sentetik dataset dizini
        """
        self.dataset_dir = Path(dataset_dir)
        self.labels_file = self.dataset_dir / "labels.csv"
        self.feature_extractor = AudioFeatureExtractor()
        
        # Model tanÄ±mlarÄ±
        self.models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                random_state=42
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ),
            'SVM': SVC(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                probability=True,
                random_state=42
            ),
            'LogisticRegression': LogisticRegression(
                C=1.0,
                max_iter=1000,
                random_state=42
            )
        }
        
        self.results = {}
        
    def load_dataset(self):
        """Dataset ve etiketleri yÃ¼kle"""
        if not self.labels_file.exists():
            raise FileNotFoundError(f"Labels dosyasÄ± bulunamadÄ±: {self.labels_file}")
        
        print(f"ğŸ“Š Dataset yÃ¼kleniyor: {self.dataset_dir}")
        
        # Labels'Ä± yÃ¼kle
        self.labels_df = pd.read_csv(self.labels_file)
        
        print(f"âœ… {len(self.labels_df)} Ã¶rnek yÃ¼klendi")
        print(f"ğŸ“ˆ SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        
        class_counts = self.labels_df['class'].value_counts()
        for class_name, count in class_counts.items():
            print(f"   {class_name}: {count} Ã¶rnek")
        
        return self.labels_df
    
    def extract_features_from_dataset(self, force_recompute=False):
        """
        Dataset'ten Ã¶zellik Ã§Ä±kar
        
        Args:
            force_recompute (bool): Ã–zellikler varsa yeniden hesapla
        """
        features_file = self.dataset_dir / "features.csv"
        features_raw_file = self.dataset_dir / "features_raw.pkl"
        
        if features_file.exists() and not force_recompute:
            print(f"âœ… Ã–zellikler zaten mevcut: {features_file}")
            features_df = pd.read_csv(features_file)
            return features_df
        
        print(f"ğŸ”¬ Ã–zellik Ã§Ä±karÄ±mÄ± baÅŸlÄ±yor...")
        
        all_features = []
        failed_files = []
        
        for idx, row in self.labels_df.iterrows():
            audio_file = self.dataset_dir / row['filename']
            
            if not audio_file.exists():
                print(f"âŒ Dosya bulunamadÄ±: {audio_file}")
                failed_files.append(row['filename'])
                continue
            
            try:
                # Ã–zellik Ã§Ä±kar
                features = self.feature_extractor.extract_all_features(str(audio_file))
                
                # Metadata ekle
                features['filename'] = row['filename']
                features['class'] = row['class']
                features['text'] = row['text']
                
                all_features.append(features)
                
                if (idx + 1) % 50 == 0:
                    print(f"   âœ… {idx+1}/{len(self.labels_df)} iÅŸlendi")
                    
            except Exception as e:
                print(f"âŒ Ã–zellik Ã§Ä±karma hatasÄ± {audio_file}: {e}")
                failed_files.append(row['filename'])
                continue
        
        if not all_features:
            raise ValueError("HiÃ§bir dosyadan Ã¶zellik Ã§Ä±karÄ±lamadÄ±!")
        
        # DataFrame oluÅŸtur
        features_df = pd.DataFrame(all_features)
        
        # Kaydet
        features_df.to_csv(features_file, index=False)
        
        # Raw features'Ä± da kaydet (pickle ile)
        with open(features_raw_file, 'wb') as f:
            pickle.dump(all_features, f)
        
        print(f"âœ… Ã–zellik Ã§Ä±karÄ±mÄ± tamamlandÄ±!")
        print(f"ğŸ“Š BaÅŸarÄ±lÄ±: {len(all_features)}")
        print(f"âŒ BaÅŸarÄ±sÄ±z: {len(failed_files)}")
        print(f"ğŸ’¾ Ã–zellikler kaydedildi: {features_file}")
        
        if failed_files:
            print(f"âš ï¸ BaÅŸarÄ±sÄ±z dosyalar: {failed_files[:5]}...")
        
        return features_df
    
    def prepare_data(self, features_df):
        """EÄŸitim iÃ§in veri hazÄ±rla"""
        print(f"ğŸ”§ Veri hazÄ±rlÄ±ÄŸÄ±...")
        
        # Ã–zellik sÃ¼tunlarÄ±nÄ± belirle (numeric olanlar)
        feature_columns = []
        for col in features_df.columns:
            if col not in ['filename', 'class', 'text'] and features_df[col].dtype in ['float64', 'int64']:
                feature_columns.append(col)
        
        print(f"ğŸ“Š {len(feature_columns)} Ã¶zellik seÃ§ildi")
        
        # X ve y oluÅŸtur
        X = features_df[feature_columns].fillna(0)  # NaN deÄŸerleri 0 ile doldur
        y = features_df['class']
        
        # Label encoding
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"ğŸ·ï¸ SÄ±nÄ±flar: {self.label_encoder.classes_}")
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        print(f"ğŸ“ˆ EÄŸitim seti: {len(X_train)} Ã¶rnek")
        print(f"ğŸ“ˆ Test seti: {len(X_test)} Ã¶rnek")
        
        return X_train, X_test, y_train, y_test, feature_columns
    
    def train_models(self, X_train, X_test, y_train, y_test):
        """Modelleri eÄŸit"""
        print(f"ğŸ¤– Model eÄŸitimi baÅŸlÄ±yor...")
        
        self.trained_models = {}
        self.scalers = {}
        
        for model_name, model in self.models.items():
            print(f"\nğŸ¯ {model_name} eÄŸitiliyor...")
            
            # Scaling iÃ§in pipeline oluÅŸtur
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('model', model)
            ])
            
            # EÄŸit
            pipeline.fit(X_train, y_train)
            
            # Test et
            y_pred = pipeline.predict(X_test)
            y_prob = pipeline.predict_proba(X_test) if hasattr(pipeline, 'predict_proba') else None
            
            # Metrikler
            accuracy = accuracy_score(y_test, y_pred)
            
            # Cross-validation (adaptif fold sayÄ±sÄ±)
            from collections import Counter
            class_counts = Counter(y_train)
            min_class_count = min(class_counts.values())
            n_folds = min(3, min_class_count)  # En az 2, en fazla 3 fold
            if n_folds < 2:
                n_folds = 2
            
            try:
                cv_scores = cross_val_score(pipeline, X_train, y_train, cv=n_folds)
            except ValueError:
                # EÄŸer hala hata varsa, sadece accuracy kullan
                cv_scores = np.array([accuracy])
                print(f"   âš ï¸ CV skipped due to small dataset")
            
            # SonuÃ§larÄ± kaydet
            self.results[model_name] = {
                'model': pipeline,
                'accuracy': accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'y_pred': y_pred,
                'y_prob': y_prob,
                'classification_report': classification_report(
                    y_test, y_pred, 
                    target_names=self.label_encoder.classes_,
                    output_dict=True
                )
            }
            
            self.trained_models[model_name] = pipeline
            
            print(f"   âœ… Accuracy: {accuracy:.3f}")
            print(f"   ğŸ”„ CV: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
        
        return self.results
    
    def evaluate_models(self, X_test, y_test):
        """Modelleri deÄŸerlendir"""
        print(f"\nğŸ“Š Model deÄŸerlendirmesi...")
        
        # SonuÃ§larÄ± Ã¶zetle
        results_summary = []
        
        for model_name, result in self.results.items():
            results_summary.append({
                'Model': model_name,
                'Test Accuracy': result['accuracy'],
                'CV Mean': result['cv_mean'],
                'CV Std': result['cv_std']
            })
        
        results_df = pd.DataFrame(results_summary)
        results_df = results_df.sort_values('Test Accuracy', ascending=False)
        
        print(f"\nğŸ† Model KarÅŸÄ±laÅŸtÄ±rmasÄ±:")
        print(results_df.to_string(index=False, float_format='%.3f'))
        
        # En iyi modeli belirle
        best_model_name = results_df.iloc[0]['Model']
        print(f"\nğŸ¥‡ En iyi model: {best_model_name}")
        
        return results_df, best_model_name
    
    def create_visualizations(self, X_test, y_test, output_dir=None):
        """GÃ¶rselleÅŸtirmeler oluÅŸtur"""
        if output_dir is None:
            output_dir = self.dataset_dir / "model_results"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ˆ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
        
        # 1. Model karÅŸÄ±laÅŸtÄ±rmasÄ±
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Accuracy karÅŸÄ±laÅŸtÄ±rmasÄ±
        model_names = list(self.results.keys())
        accuracies = [self.results[name]['accuracy'] for name in model_names]
        cv_means = [self.results[name]['cv_mean'] for name in model_names]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        axes[0].bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8)
        axes[0].bar(x + width/2, cv_means, width, label='CV Mean', alpha=0.8)
        axes[0].set_xlabel('Models')
        axes[0].set_ylabel('Accuracy')
        axes[0].set_title('Model Performance Comparison')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(model_names, rotation=45)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Confusion matrix (en iyi model iÃ§in)
        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])
        best_result = self.results[best_model_name]
        
        cm = confusion_matrix(y_test, best_result['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.label_encoder.classes_,
                   yticklabels=self.label_encoder.classes_,
                   ax=axes[1])
        axes[1].set_title(f'Confusion Matrix - {best_model_name}')
        axes[1].set_xlabel('Predicted')
        axes[1].set_ylabel('Actual')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. SÄ±nÄ±f bazÄ±nda performans
        fig, ax = plt.subplots(figsize=(10, 6))
        
        class_report = best_result['classification_report']
        classes = self.label_encoder.classes_
        
        metrics = ['precision', 'recall', 'f1-score']
        x = np.arange(len(classes))
        width = 0.25
        
        for i, metric in enumerate(metrics):
            values = [class_report[cls][metric] for cls in classes]
            ax.bar(x + i*width, values, width, label=metric.title(), alpha=0.8)
        
        ax.set_xlabel('Classes')
        ax.set_ylabel('Score')
        ax.set_title(f'Per-Class Performance - {best_model_name}')
        ax.set_xticks(x + width)
        ax.set_xticklabels(classes)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'class_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š GÃ¶rselleÅŸtirmeler kaydedildi: {output_dir}")
        
        return output_dir
    
    def save_best_model(self, best_model_name, output_dir=None):
        """En iyi modeli kaydet"""
        if output_dir is None:
            output_dir = self.dataset_dir / "trained_models"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True)
        
        best_model = self.trained_models[best_model_name]
        
        # Model ve metadata'yÄ± kaydet
        model_data = {
            'model': best_model,
            'label_encoder': self.label_encoder,
            'model_name': best_model_name,
            'training_date': datetime.now().isoformat(),
            'performance': self.results[best_model_name],
            'dataset_info': str(self.dataset_dir)
        }
        
        model_file = output_dir / f"best_model_{best_model_name.lower()}.pkl"
        
        with open(model_file, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ En iyi model kaydedildi: {model_file}")
        
        # TÃ¼m sonuÃ§larÄ± da kaydet
        results_file = output_dir / "training_results.json"
        
        # JSON serializable hale getir
        json_results = {}
        for name, result in self.results.items():
            json_results[name] = {
                'accuracy': float(result['accuracy']),
                'cv_mean': float(result['cv_mean']),
                'cv_std': float(result['cv_std']),
                'classification_report': result['classification_report']
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ EÄŸitim sonuÃ§larÄ± kaydedildi: {results_file}")
        
        return model_file, results_file

def main():
    """Ana fonksiyon"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Sentetik Dataset ile Model EÄŸitimi")
    parser.add_argument('--dataset', '-d', required=True,
                       help='Sentetik dataset dizini')
    parser.add_argument('--output', '-o', 
                       help='Ã‡Ä±ktÄ± dizini (varsayÄ±lan: dataset/model_results)')
    parser.add_argument('--force-features', '-f', action='store_true',
                       help='Ã–zellik Ã§Ä±karÄ±mÄ±nÄ± yeniden yap')
    parser.add_argument('--no-viz', action='store_true',
                       help='GÃ¶rselleÅŸtirme oluÅŸturma')
    
    args = parser.parse_args()
    
    try:
        # Model eÄŸitici
        trainer = SyntheticModelTrainer(args.dataset)
        
        # Dataset yÃ¼kle
        labels_df = trainer.load_dataset()
        
        # Ã–zellik Ã§Ä±kar
        features_df = trainer.extract_features_from_dataset(
            force_recompute=args.force_features
        )
        
        # Veri hazÄ±rla
        X_train, X_test, y_train, y_test, feature_columns = trainer.prepare_data(features_df)
        
        # Modelleri eÄŸit
        results = trainer.train_models(X_train, X_test, y_train, y_test)
        
        # DeÄŸerlendir
        results_df, best_model_name = trainer.evaluate_models(X_test, y_test)
        
        # GÃ¶rselleÅŸtir
        if not args.no_viz:
            viz_dir = trainer.create_visualizations(X_test, y_test, args.output)
        
        # En iyi modeli kaydet
        model_file, results_file = trainer.save_best_model(best_model_name, args.output)
        
        print(f"\nâœ… Model eÄŸitimi tamamlandÄ±!")
        print(f"ğŸ† En iyi model: {best_model_name}")
        print(f"ğŸ’¾ Model dosyasÄ±: {model_file}")
        print(f"ğŸ“Š SonuÃ§lar: {results_file}")
        
        # Test iÃ§in Ã¶rnek kod
        print(f"\nğŸ§ª Test iÃ§in Ã¶rnek kod:")
        print(f"```python")
        print(f"import pickle")
        print(f"from feature_extraction import AudioFeatureExtractor")
        print(f"")
        print(f"# Model yÃ¼kle")
        print(f"with open('{model_file}', 'rb') as f:")
        print(f"    model_data = pickle.load(f)")
        print(f"")
        print(f"model = model_data['model']")
        print(f"label_encoder = model_data['label_encoder']")
        print(f"")
        print(f"# Yeni ses dosyasÄ± tahmin et")
        print(f"extractor = AudioFeatureExtractor()")
        print(f"features = extractor.extract_all_features('test.wav')")
        print(f"")
        print(f"# Ã–zellik vektorÃ¼ hazÄ±rla")
        print(f"feature_vector = [features[col] for col in feature_columns]")
        print(f"prediction = model.predict([feature_vector])")
        print(f"predicted_class = label_encoder.inverse_transform(prediction)[0]")
        print(f"```")
        
    except Exception as e:
        print(f"âŒ Hata: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 